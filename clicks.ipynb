{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clicks-only dataset creation\n",
    "This notebook describes pre-processing of clicks data to create reliable datasets.\n",
    "\n",
    "Both original and reproducibility papers only mention **clicks** and metrics related to clicks,  \n",
    "however related codebase merge **buys** data to **clicks** data. This have  \n",
    "positive and negative effects. Positive one is that it prolongs some sessions - i.e. adds more  \n",
    "user actions in form of buy events. On the other hand, buy events are likely not ordered and  \n",
    "are added to existing sessions as en extra items while it is not clear how were they recorded.  \n",
    "Buys only represent small portion of data (rc15 has 1,110,965 clicks and only 43,946 buys).  \n",
    "Moreover, published code treats rewards for clicks and buys  \n",
    "differently, which is not described in text of the papers. As reasons for merging **buys**  \n",
    "data to **clicks** dataset is not clear and potentially brings more issues I decided to  \n",
    "benchmark models with clicks-only dataset.\n"
   ],
   "id": "386ca20ecea22d72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from src.utils import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "data_path = cur_dir + '/div4rec/rc15_data/'\n",
    "data_path_save = cur_dir + '/div4rec/rc15_data/Clicks_only/'\n",
    "os.makedirs(data_path_save, exist_ok=True)"
   ],
   "id": "ead257e42c087583",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Start with sampled_buys and sampled_clicks\n",
    "We start with the data produced by sample_data_rc15.py. At least here we copy original work.  \n",
    "However, we omit sampled_buys.fd and continue with sampled_clicks.df only. Following code    \n",
    "partially corresponds to merge_and_sort_rc15.py, but we skip merging buys into clicks  \n",
    "and only sort clicks data."
   ],
   "id": "777cb3610c09fafb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sampled_clicks = pd.read_pickle(os.path.join(data_path, 'sampled_clicks.df'))\n",
    "sampled_clicks=sampled_clicks.drop(columns=['category'])\n",
    "sampled_clicks['is_buy']=0\n",
    "sampled_clicks=sampled_clicks.sort_values(by=['session_id','timestamp'])\n",
    "\n",
    "sampled_clicks.to_csv(f'{data_path_save}sampled_clicks.csv', index = None, header=True)\n",
    "to_pickled_df(data_path_save, sampled_clicks=sampled_clicks)"
   ],
   "id": "34d48b056ebef91e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Continue with split_data.py\n",
    "Almost no changes, we take our sorted clicks data and split it to train, test and val."
   ],
   "id": "86b0e6bf06f59ee4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_sessions=sampled_clicks.session_id.unique()\n",
    "np.random.shuffle(total_sessions)\n",
    "\n",
    "fractions = np.array([0.8, 0.1, 0.1])\n",
    "# split into 3 parts\n",
    "train_ids, val_ids, test_ids = np.array_split(\n",
    "    total_sessions, (fractions[:-1].cumsum() * len(total_sessions)).astype(int))\n",
    "train_sessions=sampled_clicks[sampled_clicks['session_id'].isin(train_ids)]\n",
    "val_sessions=sampled_clicks[sampled_clicks['session_id'].isin(val_ids)]\n",
    "test_sessions=sampled_clicks[sampled_clicks['session_id'].isin(test_ids)]\n",
    "\n",
    "to_pickled_df(data_path_save, sampled_train=train_sessions)\n",
    "to_pickled_df(data_path_save, sampled_val=val_sessions)\n",
    "to_pickled_df(data_path_save, sampled_test=test_sessions)"
   ],
   "id": "9d2a957f050671cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generate replay buffers for click data\n",
    "Here we generate replay buffers for test, val and train datasets. Replay buffer  \n",
    "is source of data further processed before loading by DataLoader.\n",
    "\n",
    "replay_buffer entry (line) has following format:  \n",
    "0 &nbsp;&nbsp;&nbsp; [26702, 26702, 26702, 26702, 26702, 26702, 26702, 26702, 26702, 26702]&nbsp;&nbsp;&nbsp; 1 &nbsp; &nbsp; &nbsp; 217 &nbsp;&nbsp;&nbsp; 0 &nbsp;&nbsp;&nbsp; [217, 26702, 26702, 26702, 26702, 26702, 26702, 26702, 26702, 26702] &nbsp;&nbsp;&nbsp; 1 &nbsp;&nbsp;&nbsp; False  \n",
    "where left to right: line number; **state**; len_state; action; is_buy; **next_state**; len_next_state; is_done  \n",
    "Note that 26702 is padding item and state is thus empty sequence.\n",
    "\n",
    "We follow two variants:\n",
    " - original approach - buffer contains lines with \"empty\" **state** (only padding items) - see replay buffer example above\n",
    " - improved approach - 1st state in buffer already contains one or more items\n",
    "\n",
    "Explainer: with original approach all buffers contain lines with no item in **state** and  \n",
    "single item in **next_state**. That means model should guess first item during inference  \n",
    "and consequently some metrics (as hit ration) are affected by considering these random guesses.  \n",
    "My estimation is that 10 - 20 % of entries in replay buffers are of this kind and I expect that  \n",
    "having around 26700 items it leads to truly random guesses. Improved dataset will eliminate  \n",
    "situations when model should infer (guess) based on empty (paddings only) sequence. Training  \n",
    "may also be negatively affected by empty states in training data."
   ],
   "id": "cd2bf108d90910f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "state_size, item_num = get_stats(data_path)\n",
    "\n",
    "def create_buffer(dataset_name, sorted_events, output_path, skip_length=0):\n",
    "    pad_item = item_num\n",
    "    groups = sorted_events.groupby('session_id')\n",
    "    ids = sorted_events.session_id.unique()\n",
    "\n",
    "    state, len_state, action, is_buy, next_state, len_next_state, is_done = [], [], [], [], [],[],[]\n",
    "\n",
    "    for id in ids:\n",
    "        group = groups.get_group(id)\n",
    "        history = []\n",
    "        for index, row in group.iterrows():\n",
    "            s = list(history)\n",
    "            s = pad_history(s, state_size, pad_item)\n",
    "            a = row['item_id']\n",
    "            is_b = row['is_buy']\n",
    "            history.append(row['item_id'])\n",
    "            next_s = list(history)\n",
    "            next_s=pad_history(next_s, state_size, pad_item)\n",
    "            # sequences of skip_length or shorter are not added to dataset\n",
    "            if len(history) > skip_length:\n",
    "                state.append(s)\n",
    "                len_state.append(state_size if len(s) >= state_size else 1 if len(s) == 0 else len(s))\n",
    "                action.append(a)\n",
    "                is_buy.append(is_b)\n",
    "                len_next_state.append(state_size if len(next_s) >= state_size else 1 if len(next_s) == 0 else len(next_s))\n",
    "                next_state.append(next_s)\n",
    "                is_done.append(False)\n",
    "        is_done[-1] = True\n",
    "\n",
    "    replay_buffer_dict = {\n",
    "        'state': state,\n",
    "        'len_state': len_state,\n",
    "        'action': action,\n",
    "        'is_buy': is_buy,\n",
    "        'next_state': next_state,\n",
    "        'len_next_states': len_next_state,\n",
    "        'is_done': is_done\n",
    "    }\n",
    "    replay_buffer = pd.DataFrame(data=replay_buffer_dict)\n",
    "    replay_buffer.to_pickle(output_path + f'replay_buffer_{dataset_name}_skip={skip_length}.df')"
   ],
   "id": "e5398335ee328995",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for dataset in ['val', 'test', 'train']:\n",
    "    sorted_events = pd.read_pickle(data_path_save + f'sampled_{dataset}.df')\n",
    "    for skip in [0, 1, 2]:\n",
    "        create_buffer(dataset, sorted_events, data_path_save, skip)"
   ],
   "id": "e2c970db97e84ba4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "328b632aa6ca2029"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
